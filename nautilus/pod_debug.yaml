apiVersion: v1
kind: Pod
metadata:
  name: zihao-pod-debug # replace with "${USER_NAME}-{POD_NAME}"
  namespace: ucsd-haosulab
spec:
  containers:
    - name: gpu-container
      image: nvidia/cuda:12.1.1-devel-ubuntu22.04 # docker image
      # imagePullPolicy: Always
      args: ["sleep", "infinity"]
      resources:
        requests:
          cpu: "1"
          memory: "4Gi"
          nvidia.com/gpu: "1"
        limits:
          cpu: "2"
          memory: "8Gi"
          nvidia.com/gpu: "1"      
      volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: zihao-fast-vol # change this based on your own pvc
          mountPath: /zihao-fast-vol # change this based on your own pvc
  volumes:
    - name: dshm # shared memory, required for the multi-worker dataloader
      emptyDir:
        medium: Memory
    - name: zihao-fast-vol # change this based on your own pvc
      persistentVolumeClaim:
        claimName: zihao-fast-vol # change this based on your own pvc; you can get the pvc name of your created volume from "kubectl get pvc"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: nautilus.io/group
                operator: In
                values:
                  - haosu
              - key: nvidia.com/gpu.product
                operator: In
                values:
                  - NVIDIA-GeForce-RTX-4090
                  - NVIDIA-GeForce-RTX-3090
                  - NVIDIA-GeForce-RTX-2080-Ti
                  - NVIDIA-RTX-A6000
                  - NVIDIA-A10
